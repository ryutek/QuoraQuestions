{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train and test data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import lightgbm as lgb\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def distance_lat_lon(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c   \n",
    "    return distance\n",
    "\n",
    "\n",
    "print(\"Read train and test data...\")\n",
    "train_df = pd.read_json(\"../input/train.json\")\n",
    "test_df = pd.read_json(\"../input/test.json\")\n",
    "\n",
    "img_time = pd.read_csv(\"../input/listing_image_time.csv\")\n",
    "img_time.columns = ['listing_id', 'time_stamp']\n",
    "train_df = pd.merge(train_df, img_time,  how='left', on=['listing_id'])\n",
    "test_df = pd.merge(test_df, img_time,  how='left', on=['listing_id'])\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=1800):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.021\n",
    "    param['max_depth'] = 6\n",
    "    #param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=30)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    \n",
    "    print(\"Predict test...\")\n",
    "    print(test_X.shape)\n",
    "    print(model)\n",
    "    #print(test_X.head())\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    #pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "def runLGBM(train_X, train_y, test_X, test_y=None, feature_names=None, num_rounds=1800):\n",
    "    params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_classes': 3,\n",
    "    'metric': {'multi_logloss'},\n",
    "    'num_leaves': 55,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.82,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "    }\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    #plst = list(param.items())\n",
    "    #train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    #val_data = lgb.Dataset(X_val[features], y_val)\n",
    "    \n",
    "    train_data = lgb.Dataset(train_X, label=train_y)\n",
    "    '''\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    '''\n",
    "    print(\"Predict cv...\")\n",
    "    bstcv = lgb.cv(params, train_data, num_rounds, verbose_eval = 100, early_stopping_rounds = 30)\n",
    "    #print(test_X.shape)\n",
    "    #print(model)\n",
    "    #print(test_X.head())\n",
    "    #pred_test_y = model.predict(test_X)\n",
    "    #pred_test_y = model.predict(xgtest)\n",
    "    return bstcv #pred_test_y, model\n",
    "\n",
    "def add_median_price(key=None, suffix=\"\", trn_df=None, tst_df=None):\n",
    "    \"\"\"\n",
    "    Compute median prices for renthop dataset.\n",
    "    The function adds 2 columns to the pandas DataFrames : the median prices and a ratio\n",
    "    between nthe actual price of the rent and the median\n",
    "    \n",
    "    :param key: list of columns on which to groupby and compute median prices\n",
    "    :param suffix: string used to suffix the newly created columns/features\n",
    "    :param trn_df: training dataset as a pandas DataFrame\n",
    "    :param tst_df: test dataset as a pandas DataFrame\n",
    "    :return: updated train and test DataFrames\n",
    "\n",
    "    :Example\n",
    "    \n",
    "    train, test = add_median_price(key=['bedrooms', 'bathrooms'], \n",
    "                                   suffix='rooms', \n",
    "                                   trn_df=train, \n",
    "                                   tst_df=test)\n",
    "\n",
    "    \"\"\"\n",
    "    # Set features to be used\n",
    "    median_features = key[:]\n",
    "    median_features.append('price')\n",
    "    # Concat train and test to find median prices over whole dataset\n",
    "    median_prices = pd.concat([trn_df[median_features], tst_df[median_features]], axis=0)\n",
    "    # Group data by key to compute median prices\n",
    "    medians_by_key = median_prices.groupby(by=key)['price'].median().reset_index()\n",
    "    # Rename median column with provided suffix\n",
    "    medians_by_key.rename(columns={'price': 'median_price_' + suffix}, inplace=True)\n",
    "    # Update data frames, note that merge seems to reset the index\n",
    "    # that's why I reset first and set again the index\n",
    "    #trn_df = trn_df.reset_index().merge(medians_by_key, on=key, how='left').set_index('listing_id')\n",
    "    #tst_df = tst_df.reset_index().merge(medians_by_key, on=key, how='left').set_index('listing_id')\n",
    "    \n",
    "    trn_df = pd.merge(trn_df, medians_by_key, how = 'left', on = key)\n",
    "    tst_df = pd.merge(tst_df, medians_by_key, how = 'left', on = key)\n",
    "    \n",
    "    trn_df['price_to_median_ratio_' + suffix] = trn_df['price'] /trn_df['median_price_' + suffix]\n",
    "    tst_df['price_to_median_ratio_' + suffix] = tst_df['price'] / tst_df['median_price_' + suffix]\n",
    "\n",
    "    return trn_df, tst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering...\n",
      "   bathrooms  bedrooms  latitude  longitude  price  price_t  price_per_room  \\\n",
      "0        1.5         3   40.7145   -73.9425   3000   1000.0      666.666667   \n",
      "1        1.0         2   40.7947   -73.9667   5465   2732.5     1821.666667   \n",
      "2        1.0         1   40.7388   -74.0018   2850   2850.0     1425.000000   \n",
      "3        1.0         1   40.7539   -73.9677   3275   3275.0     1637.500000   \n",
      "4        1.0         4   40.8241   -73.9493   3350    837.5      670.000000   \n",
      "\n",
      "   logprice  density  half_bathrooms      ...       num_rot30_Y  num_rot45_X  \\\n",
      "0  8.006368        5             0.5      ...        -84.393333   -23.495744   \n",
      "1  8.606119       62             0.0      ...        -84.454391   -23.456146   \n",
      "2  7.955074       92             0.0      ...        -84.456839   -23.520493   \n",
      "3  8.094073      144             0.0      ...        -84.434857   -23.485703   \n",
      "4  8.116716        5             0.0      ...        -84.454022   -23.423054   \n",
      "\n",
      "   num_rot45_Y  num_rot60_X  num_rot60_Y  num_cap_share  num_nr_of_lines  \\\n",
      "0   -81.074742   -43.678833   -72.231041       0.103565                3   \n",
      "1   -81.148564   -43.659691   -72.312597       0.000000                0   \n",
      "2   -81.133856   -43.718039   -72.281736       0.026012                0   \n",
      "3   -81.120421   -43.680957   -72.277763       0.048682                9   \n",
      "4   -81.157049   -43.629922   -72.329358       0.025000                0   \n",
      "\n",
      "   num_redacted  num_email  num_phone_nr  \n",
      "0             1          0             1  \n",
      "1             0          0             0  \n",
      "2             1          0             0  \n",
      "3             1          0             0  \n",
      "4             0          0             0  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "Convert categorical to numeric...\n",
      "['street_address', 'display_address', 'manager_id', 'building_id', 'room_combi']\n",
      "tfidf for features\n",
      "Feature engineering is complete!\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "print(\"Feature engineering...\")\n",
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "\n",
    "train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "test_df[\"created_week\"] = test_df[\"created\"].dt.week\n",
    "\n",
    "'''\n",
    "# median price by group\n",
    "median_features = ['bedrooms', 'price']\n",
    "median_prices = pd.concat([train_df[median_features], test_df[median_features]], axis=0)\n",
    "medians_by_key = median_prices.groupby(by=['bedrooms'])['price'].median().reset_index()\n",
    "medians_by_key.rename(columns={'price': 'median_price_bed'}, inplace=True)\n",
    "train_df = pd.merge(train_df, medians_by_key, how = 'left', on = 'bedrooms')\n",
    "test_df = pd.merge(test_df, medians_by_key, how = 'left', on = 'bedrooms')\n",
    "train_df['price_to_median_ratio_bed'] = train_df['price'] /train_df['median_price_bed']\n",
    "test_df['price_to_median_ratio_bed'] = test_df['price'] /test_df['median_price_bed']\n",
    "'''\n",
    "\n",
    "train_df, test_df = add_median_price(key=['bedrooms'], suffix=\"bed\", trn_df=train_df, tst_df=test_df)\n",
    "train_df, test_df = add_median_price(key=['building_id'], suffix=\"building\", trn_df=train_df, tst_df=test_df)\n",
    "train_df, test_df = add_median_price(key=['manager_id'], suffix=\"manager\", trn_df=train_df, tst_df=test_df)\n",
    "\n",
    "#train_df, test_df = add_median_price(key=['building_id', 'manager_id'], suffix=\"bld_mg\", trn_df=train_df, tst_df=test_df)\n",
    "train_df, test_df = add_median_price(key=['bedrooms', 'manager_id'], suffix=\"bed_mg\", trn_df=train_df, tst_df=test_df)\n",
    "train_df, test_df = add_median_price(key=['bedrooms', 'building_id'], suffix=\"bed_bld\", trn_df=train_df, tst_df=test_df)\n",
    "\n",
    "def distance_lat_lon_cols(_df, ny_lat, ny_lon, _lat, _lon):\n",
    "    dist = []\n",
    "    for lat, lon in zip(_df[_lat].values, _df[_lon].values):\n",
    "        distance = distance_lat_lon(ny_lat, ny_lon, lat, lon)\n",
    "        dist.append(distance)\n",
    "    return dist\n",
    "\n",
    "# New York City Center Coords\n",
    "ny_lat = 40.785091\n",
    "ny_lon = -73.968285\n",
    "\n",
    "train_df[\"distance\"] = distance_lat_lon_cols(train_df, ny_lat, ny_lon, 'latitude', 'longitude')\n",
    "test_df[\"distance\"] = distance_lat_lon_cols(test_df, ny_lat, ny_lon, 'latitude', 'longitude')\n",
    "\n",
    "train_df[\"total_days\"] =(train_df[\"created_month\"] -4.0)*30 + train_df[\"created_day\"] +  train_df[\"created_hour\"] /25.0\n",
    "test_df[\"total_days\"] =(test_df[\"created_month\"] -4.0)*30 + test_df[\"created_day\"] +  test_df[\"created_hour\"] /25.0        \n",
    "train_df[\"diff_rank\"]= train_df[\"total_days\"]/train_df[\"listing_id\"]\n",
    "test_df[\"diff_rank\"]= test_df[\"total_days\"]/test_df[\"listing_id\"]\n",
    "\n",
    "\n",
    "# Add column with bool values to check if keyword is contained or not\n",
    "def containColumn(_df, _col, _str):\n",
    "    string_name = _str.lower().replace(' ', '_')\n",
    "    _df[_col+'_'+string_name] = _df[_col].apply(lambda x: _str.lower() in x.lower())\n",
    "    return _df\n",
    "\n",
    "# Add column with value count\n",
    "def valueCountColumn(_df, _col):\n",
    "    _dict = dict([(i, a) for i, a in zip(_df[_col].value_counts().index, _df[_col].value_counts().values)])\n",
    "    _df[_col+'ValueCount'] = _df[_col].apply(lambda x: _dict[x])\n",
    "    return _df\n",
    "\n",
    "# group by country and campaign id\n",
    "def countGroupBy(_df, str1, str2):\n",
    "    new_col = str1 + \"_\" + str2\n",
    "    _df[new_col] = _df[[str1, str2]].apply(lambda x : '{}-{}'.format(x[0],x[1]), axis=1)\n",
    "    _df = valueCountColumn(_df, new_col)\n",
    "    _df.drop(new_col, axis=1, inplace=True)\n",
    "    print(\"New value count column is added:\", new_col)\n",
    "    return _df\n",
    "\n",
    "\n",
    "import string\n",
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "def feature_engineering(df):\n",
    "    #df['md'] = df.created_month.astype(str) + '_' + df.created_day.astype(str)\n",
    "    #df['mdh'] = df.md.astype(str) + '_' + df.created_hour.astype(str)\n",
    "    \n",
    "    # room combination type\n",
    "    df['room_combi'] = df.bedrooms.astype(str) + '_' + df.bathrooms.astype(str)\n",
    "    \n",
    "    '''\n",
    "    # address\n",
    "    df['address1'] = df['display_address']\n",
    "    df['address1'] = df['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "    address_map = {\n",
    "        'w': 'west',\n",
    "        'st.': 'street',\n",
    "        'ave': 'avenue',\n",
    "        'st': 'street',\n",
    "        'e': 'east',\n",
    "        'n': 'north',\n",
    "        's': 'south'\n",
    "    }\n",
    "    \n",
    "\n",
    "    def address_map_func(s):\n",
    "        s = s.split(' ')\n",
    "        out = []\n",
    "        for x in s:\n",
    "            if x in address_map:\n",
    "                out.append(address_map[x])\n",
    "            else:\n",
    "                out.append(x)\n",
    "        return ' '.join(out)\n",
    "\n",
    "    df['address1'] = df['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    df['address1'] = df['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "    new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "    for col in new_cols:\n",
    "        df[col] = df['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "    df['other_address'] = df[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "    \n",
    "    df.drop('address1', axis = 1, inplace = True)\n",
    "    '''\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "#train_df['room_combi'] = train_df.bedrooms.astype(str) + '_' + train_df.bathrooms.astype(str)\n",
    "#test_df['room_combi'] = test_df.bedrooms.astype(str) + '_' + test_df.bathrooms.astype(str)\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "\n",
    "import math\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    for df in [tr_df, te_df]:\n",
    "        #polar coordinates system\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df\n",
    "\n",
    "train_df, test_df = operate_on_coordinates(train_df, test_df)\n",
    "\n",
    "import re\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "    df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "    \n",
    "    # how long in lines the desc is?\n",
    "    df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "   \n",
    "    # is the description redacted by the website?        \n",
    "    df['num_redacted'] = 0\n",
    "    df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "    \n",
    "    # can we contact someone via e-mail to ask for the details?\n",
    "    df['num_email'] = 0\n",
    "    df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "    \n",
    "    #and... can we call them?\n",
    "    \n",
    "    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "    def try_and_find_nr(description):\n",
    "        if reg.match(description) is None:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "\n",
    "'''    \n",
    "#sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def description_sentiment(sentences):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    return pd.DataFrame(result).mean()\n",
    "\n",
    "train_df['description_tokens'] = train_df['description'].apply(sent_tokenize)\n",
    "train_df = pd.concat([train_df, train_df['description_tokens'].apply(description_sentiment)],axis=1)\n",
    "\n",
    "test_df['description_tokens'] = test_df['description'].apply(sent_tokenize)\n",
    "test_df = pd.concat([train_df, test_df['description_tokens'].apply(description_sentiment)],axis=1)\n",
    "'''\n",
    "\n",
    "#missing values\n",
    "train_df = train_df.fillna(-999).replace(np.inf, -999)\n",
    "test_df = test_df.fillna(-999).replace(np.inf, -999)\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\", \n",
    "                 \"half_bathrooms\", \"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \n",
    "                 \"created_month\", \"created_day\", \"created_hour\", \"created_week\", \"created_weekday\", \"time_stamp\",\n",
    "                 \"distance\", \"total_days\", \"diff_rank\", \n",
    "                 \"price_to_median_ratio_bed\", \"median_price_bed\",\n",
    "                 \"price_to_median_ratio_building\", \"median_price_building\",\n",
    "                 \"price_to_median_ratio_manager\", \"median_price_manager\",\n",
    "                 #\"price_to_median_ratio_bld_mg\", \"median_price_bld_mg\",\n",
    "                 \"price_to_median_ratio_bed_mg\", \"median_price_bed_mg\",\n",
    "                 \"price_to_median_ratio_bed_bld\", \"median_price_bed_bld\",\n",
    "                 \"num_rho\", \"num_phi\", \"num_rot15_X\", \"num_rot15_Y\", \"num_rot30_X\", \"num_rot30_Y\", \"num_rot45_X\",\n",
    "                 \"num_rot45_Y\", \"num_rot60_X\", \"num_rot60_Y\", 'num_cap_share', 'num_nr_of_lines',\n",
    "                 'num_redacted', 'num_email', 'num_phone_nr',\n",
    "                 #'street', 'avenue', 'east', 'west', 'north', 'south', 'other_address'\n",
    "                 #\"description_no_fee\", \"description_low_fee\", \"description_brand_new\", \n",
    "                 #\"description_location\", \"description_luxury\"\n",
    "                ]\n",
    "\n",
    "print(train_df[features_to_use].head())\n",
    "\n",
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')\n",
    "\n",
    "print(\"Convert categorical to numeric...\")\n",
    "categorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\", \"room_combi\"#, \"description_tokens\"\n",
    "              #,\"w_building_id\", \"w_manager_id\", \"w_display_address\", \"w_street_address\", \n",
    "              #,'building_id_price_roundValueCount'\n",
    "              ]\n",
    "\n",
    "print(categorical)\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "print(\"tfidf for features\")\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "tfidf_train_df = pd.DataFrame(tr_sparse.toarray())\n",
    "tfidf_train_df.columns = ['tf' + str(i) for i in range(200)]\n",
    "tfidf_test_df = pd.DataFrame(te_sparse.toarray())\n",
    "tfidf_test_df.columns = ['tf' + str(i) for i in range(200)]\n",
    "\n",
    "train_X = pd.concat([train_df[features_to_use], tfidf_train_df], axis = 1)\n",
    "test_X = pd.concat([test_df[features_to_use], tfidf_test_df], axis = 1)\n",
    "\n",
    "#train_df['description'] =  train_df['description'].apply(lambda x: str(x) if len(x)>2 else \"nulldesc\") \n",
    "#test_df['description'] = test_df['description'].apply(lambda x: str(x) if len(x)>2 else \"nulldesc\") \n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "print(\"Feature engineering is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hyunor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_to_median_ratio_bed</th>\n",
       "      <th>median_price_bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.631343</td>\n",
       "      <td>3350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.982759</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.129310</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.567797</td>\n",
       "      <td>5900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_to_median_ratio_bed  median_price_bed\n",
       "0                   0.666667            4500.0\n",
       "1                   1.631343            3350.0\n",
       "2                   0.982759            2900.0\n",
       "3                   1.129310            2900.0\n",
       "4                   0.567797            5900.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[[\"price_to_median_ratio_bed\", \"median_price_bed\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "median_features = ['bedrooms', 'price']\n",
    "median_prices = pd.concat([train_df[median_features], test_df[median_features]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8247.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms    price\n",
       "0         0   2400.0\n",
       "1         1   2900.0\n",
       "2         2   3350.0\n",
       "3         3   4500.0\n",
       "4         4   5900.0\n",
       "5         5   8109.0\n",
       "6         6   8000.0\n",
       "7         7  12000.0\n",
       "8         8   8247.5"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_prices.groupby(by=['bedrooms'])['price'].median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.7518\n",
      "-73.9779\n"
     ]
    }
   ],
   "source": [
    "print(train_df.latitude.median())\n",
    "print(train_df.longitude.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lat_median = train_df.latitude.median()\n",
    "lon_median = train_df.longitude.median()\n",
    "\n",
    "# New York City Center Coords\n",
    "ny_lat = 40.785091\n",
    "ny_lon = -73.968285\n",
    "\n",
    "dist = []\n",
    "for lat, lon in zip(train_df.latitude.values, train_df.longitude.values):\n",
    "    distance = distance_lat_lon(ny_lat, ny_lon, lat, lon)\n",
    "    dist.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49352"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.price.round(-2).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 233)\n",
      "(74659, 233)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "\n",
    "rain_df[train_df.interest_level == 'high']['description'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train_df['description'] = train_df[\"description\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "#test_df['description'] = test_df[\"description\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "'''\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=100)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"description\"])\n",
    "te_sparse = tfidf.transform(test_df[\"description\"])\n",
    "\n",
    "tfidf_train_df = pd.DataFrame(tr_sparse.toarray())\n",
    "tfidf_train_df.columns = ['tf_des_' + str(i) for i in range(100)]\n",
    "tfidf_test_df = pd.DataFrame(te_sparse.toarray())\n",
    "tfidf_test_df.columns = ['tf_des_' + str(i) for i in range(100)]\n",
    "\n",
    "train_X = pd.concat([train_X, tfidf_train_df], axis = 1)\n",
    "test_X = pd.concat([test_X, tfidf_test_df], axis = 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict cv...\n",
      "[100]\tcv_agg's multi_logloss:0.725577+0.00174373\n",
      "[200]\tcv_agg's multi_logloss:0.613911+0.00261356\n",
      "[300]\tcv_agg's multi_logloss:0.570307+0.00297516\n",
      "[400]\tcv_agg's multi_logloss:0.5491+0.00340758\n",
      "[500]\tcv_agg's multi_logloss:0.53755+0.00358496\n",
      "[600]\tcv_agg's multi_logloss:0.530231+0.00376081\n",
      "[700]\tcv_agg's multi_logloss:0.525305+0.00387723\n",
      "[800]\tcv_agg's multi_logloss:0.522023+0.00403883\n",
      "[900]\tcv_agg's multi_logloss:0.519468+0.00421874\n",
      "[1000]\tcv_agg's multi_logloss:0.517669+0.00428064\n",
      "[1100]\tcv_agg's multi_logloss:0.516376+0.00435523\n",
      "[1200]\tcv_agg's multi_logloss:0.515315+0.00448393\n",
      "[1300]\tcv_agg's multi_logloss:0.514588+0.00457409\n",
      "[1400]\tcv_agg's multi_logloss:0.514086+0.00461831\n",
      "[1500]\tcv_agg's multi_logloss:0.513741+0.0046771\n",
      "[1600]\tcv_agg's multi_logloss:0.513461+0.0048196\n",
      "[1700]\tcv_agg's multi_logloss:0.51327+0.00488071\n"
     ]
    }
   ],
   "source": [
    "#preds, model = runXGB(train_X, train_y, test_X, num_rounds=1800)\n",
    "cv_results = runLGBM(train_X, train_y, test_X, num_rounds=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      multi_logloss-mean  multi_logloss-stdv\n",
      "1690            0.513259            0.004866\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(cv_results).tail(1))\n",
    "\n",
    "best_n_estimators = len(cv_results['multi_logloss-mean'])\n",
    "best_cv_score = cv_results['multi_logloss-mean'][-1]\n",
    "\n",
    "# 0.51672\n",
    "# 0.516932    \n",
    "# 0.514822  rotations...\n",
    "# 0.513953  additional desc, email, etc\n",
    "# 0.513162  more median and street features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_features = train_X.columns.values\n",
    "\n",
    "new_features = [\n",
    "                #'street', 'avenue', 'east', 'west', 'north', 'south', 'other_address',\n",
    "                # \"price_to_median_ratio_bed\", \"median_price_bed\",\n",
    "                # \"price_to_median_ratio_building\", \"median_price_building\",\n",
    "                # \"price_to_median_ratio_manager\", \"median_price_manager\"\n",
    "    \n",
    "                 \"price_to_median_ratio_bld_mg\", \"median_price_bld_mg\",\n",
    "                 \"price_to_median_ratio_bed_mg\", \"median_price_bed_mg\",\n",
    "                 \"price_to_median_ratio_bed_bld\", \"median_price_bed_bld\"\n",
    "               ]\n",
    "\n",
    "features = list(set(all_features) - set(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "params = {\n",
    "'task': 'train',\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'multiclass',\n",
    "'num_classes': 3,\n",
    "'metric': {'multi_logloss'},\n",
    "'num_leaves': 55,\n",
    "'learning_rate': 0.01,\n",
    "'feature_fraction': 0.82,\n",
    "'bagging_fraction': 0.8,\n",
    "'bagging_freq': 5,\n",
    "'verbose': 0\n",
    "}\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1468]\tvalid_0's multi_logloss:0.506314\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1540]\tvalid_0's multi_logloss:0.506315\n",
      "('price_to_median_ratio_bld_mg', 0.50642001323308394, 0.50646521319648663, -4.5199963402686727e-05)\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1366]\tvalid_0's multi_logloss:0.506628\n",
      "('median_price_bld_mg', 0.50642001323308394, 0.50671680607703051, -0.00029679284394656946)\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1583]\tvalid_0's multi_logloss:0.506104\n",
      "('price_to_median_ratio_bed_mg', 0.50642001323308394, 0.50614914895238061, 0.00027086428070333124)\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1472]\tvalid_0's multi_logloss:0.506356\n",
      "('median_price_bed_mg', 0.50642001323308394, 0.50634366001677578, 7.6353216308167049e-05)\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1530]\tvalid_0's multi_logloss:0.506095\n",
      "('price_to_median_ratio_bed_bld', 0.50642001323308394, 0.50621863195833861, 0.00020138127474533807)\n",
      "Train until valid scores didn't improve in 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1541]\tvalid_0's multi_logloss:0.505851\n",
      "('median_price_bed_bld', 0.50642001323308394, 0.50588921524212416, 0.00053079799095978597)\n"
     ]
    }
   ],
   "source": [
    "isBenchmark = True\n",
    "\n",
    "for col in new_features:\n",
    "    if isBenchmark:\n",
    "        train_data = lgb.Dataset(X_train[features], label=y_train)\n",
    "        val_data = lgb.Dataset(X_val[features], y_val)\n",
    "        bst = lgb.train(params, train_data, 10000, valid_sets=val_data, \n",
    "                        verbose_eval = 10000, early_stopping_rounds=30)\n",
    "        pred = bst.predict(X_val[features])\n",
    "        score = log_loss(y_val, pred)  \n",
    "        benchmark = score\n",
    "        isBenchmark = False\n",
    "    \n",
    "    features.append(col)\n",
    "    train_data = lgb.Dataset(X_train[features], label=y_train)\n",
    "    val_data = lgb.Dataset(X_val[features], y_val)\n",
    "    bst = lgb.train(params, train_data, 10000, valid_sets=val_data, \n",
    "                verbose_eval = 10000, early_stopping_rounds=30)\n",
    "    pred = bst.predict(X_val[features])\n",
    "    score = log_loss(y_val, pred) \n",
    "            \n",
    "    print(col, benchmark, score, benchmark - score)\n",
    "    \n",
    "    features = list(set(features) - set([col]))\n",
    "\n",
    "#print(log_loss(y_val, pred))\n",
    "\n",
    "# t size 0.25, 0.507583\n",
    "# 0.505001 new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_day',\n",
       " 'tf41',\n",
       " 'logprice',\n",
       " 'tf74',\n",
       " 'tf55',\n",
       " 'tf54',\n",
       " 'tf57',\n",
       " 'tf56',\n",
       " 'tf51',\n",
       " 'tf50',\n",
       " 'tf53',\n",
       " 'density',\n",
       " 'tf59',\n",
       " 'tf58',\n",
       " 'created_hour',\n",
       " 'num_phi',\n",
       " u'latitude',\n",
       " 'price_per_room',\n",
       " u'building_id',\n",
       " 'tf62',\n",
       " 'tf76',\n",
       " u'bedrooms',\n",
       " 'manager_level_high',\n",
       " 'tf199',\n",
       " 'num_description_words',\n",
       " 'tf118',\n",
       " 'tf191',\n",
       " 'tf190',\n",
       " 'tf193',\n",
       " 'tf192',\n",
       " 'tf195',\n",
       " 'tf194',\n",
       " 'tf197',\n",
       " 'tf196',\n",
       " 'tf20',\n",
       " 'tf21',\n",
       " 'tf22',\n",
       " 'tf23',\n",
       " 'tf24',\n",
       " 'tf25',\n",
       " 'tf26',\n",
       " 'tf27',\n",
       " 'num_cap_share',\n",
       " 'tf29',\n",
       " 'tf72',\n",
       " 'created_year',\n",
       " 'manager_level_low',\n",
       " 'tf166',\n",
       " 'tf167',\n",
       " 'tf160',\n",
       " 'tf161',\n",
       " 'tf162',\n",
       " 'tf163',\n",
       " 'tf70',\n",
       " 'tf168',\n",
       " 'half_bathrooms',\n",
       " u'street_address',\n",
       " 'tf188',\n",
       " 'tf189',\n",
       " 'tf4',\n",
       " 'tf182',\n",
       " 'tf183',\n",
       " 'tf180',\n",
       " 'tf181',\n",
       " 'tf186',\n",
       " 'tf187',\n",
       " 'num_rot15_X',\n",
       " 'num_rot15_Y',\n",
       " 'tf33',\n",
       " 'tf32',\n",
       " 'tf31',\n",
       " 'tf30',\n",
       " 'num_rot30_Y',\n",
       " 'price_t',\n",
       " 'tf35',\n",
       " 'tf34',\n",
       " 'tf39',\n",
       " 'tf38',\n",
       " 'room_combi',\n",
       " 'tf65',\n",
       " 'created_weekday',\n",
       " 'tf110',\n",
       " 'tf113',\n",
       " 'tf112',\n",
       " 'tf115',\n",
       " 'tf114',\n",
       " 'tf117',\n",
       " 'tf116',\n",
       " 'tf119',\n",
       " 'time_stamp',\n",
       " u'price',\n",
       " 't',\n",
       " 's',\n",
       " 'tf64',\n",
       " 'num_rho',\n",
       " 'created_week',\n",
       " 'tf66',\n",
       " 'created_month',\n",
       " 'tf67',\n",
       " 'num_email',\n",
       " 'tf52',\n",
       " 'tf169',\n",
       " 'tf86',\n",
       " 'tf87',\n",
       " 'tf84',\n",
       " 'tf101',\n",
       " 'tf106',\n",
       " 'tf107',\n",
       " 'tf80',\n",
       " 'tf81',\n",
       " 'tf28',\n",
       " 'tf108',\n",
       " 'diff_rank',\n",
       " 'tf88',\n",
       " 'tf89',\n",
       " 'tf15',\n",
       " 'num_redacted',\n",
       " 'tf105',\n",
       " 'tf60',\n",
       " 'tf9',\n",
       " 'tf8',\n",
       " 'tf5',\n",
       " 'tf164',\n",
       " 'tf7',\n",
       " 'tf6',\n",
       " 'tf1',\n",
       " 'tf0',\n",
       " 'tf3',\n",
       " 'tf2',\n",
       " 'tf19',\n",
       " 'tf18',\n",
       " 'tf61',\n",
       " 'tf11',\n",
       " 'tf10',\n",
       " 'tf13',\n",
       " 'tf12',\n",
       " 'num_phone_nr',\n",
       " 'tf14',\n",
       " 'tf17',\n",
       " 'tf16',\n",
       " 'tf91',\n",
       " 'tf90',\n",
       " 'tf93',\n",
       " 'tf92',\n",
       " 'tf95',\n",
       " u'manager_id',\n",
       " 'tf97',\n",
       " 'tf96',\n",
       " 'tf137',\n",
       " 'tf98',\n",
       " 'tf135',\n",
       " 'tf134',\n",
       " 'tf133',\n",
       " 'tf132',\n",
       " 'tf131',\n",
       " 'tf94',\n",
       " 'tf159',\n",
       " 'tf139',\n",
       " 'tf155',\n",
       " 'tf154',\n",
       " 'tf157',\n",
       " 'tf156',\n",
       " 'tf151',\n",
       " 'tf150',\n",
       " 'tf153',\n",
       " 'tf152',\n",
       " 'tf99',\n",
       " 'tf165',\n",
       " 'tf136',\n",
       " 'distance',\n",
       " 'num_photos',\n",
       " 'tf63',\n",
       " u'longitude',\n",
       " 'tf128',\n",
       " 'tf129',\n",
       " 'num_nr_of_lines',\n",
       " 'r',\n",
       " 'tf68',\n",
       " 'tf69',\n",
       " 'tf120',\n",
       " 'tf121',\n",
       " 'tf122',\n",
       " 'tf123',\n",
       " 'tf124',\n",
       " 'tf125',\n",
       " 'tf126',\n",
       " 'tf127',\n",
       " u'bathrooms',\n",
       " 'tf148',\n",
       " 'tf149',\n",
       " 'tf146',\n",
       " 'tf147',\n",
       " 'tf144',\n",
       " 'tf145',\n",
       " 'tf142',\n",
       " 'tf143',\n",
       " 'tf140',\n",
       " 'tf141',\n",
       " 'tf130',\n",
       " 'tf158',\n",
       " 'tf102',\n",
       " u'listing_id',\n",
       " 'tf103',\n",
       " 'tf100',\n",
       " 'num_rot60_X',\n",
       " 'num_rot60_Y',\n",
       " 'num_rot45_Y',\n",
       " 'num_rot45_X',\n",
       " 'tf85',\n",
       " 'tf184',\n",
       " 'tf79',\n",
       " 'tf78',\n",
       " 'tf77',\n",
       " 'tf82',\n",
       " 'tf75',\n",
       " 'tf185',\n",
       " 'tf73',\n",
       " 'num_features',\n",
       " 'tf71',\n",
       " 'tf83',\n",
       " 'tf179',\n",
       " 'tf178',\n",
       " u'display_address',\n",
       " 'tf104',\n",
       " 'tf173',\n",
       " 'tf172',\n",
       " 'tf171',\n",
       " 'tf170',\n",
       " 'tf177',\n",
       " 'tf176',\n",
       " 'tf175',\n",
       " 'tf174',\n",
       " 'manager_level_medium',\n",
       " 'tf138',\n",
       " 'tf37',\n",
       " 'num_rot30_X',\n",
       " 'tf109',\n",
       " 'e',\n",
       " 'tf198',\n",
       " 'tf36',\n",
       " 'total_days',\n",
       " 'tf48',\n",
       " 'tf49',\n",
       " 'tf42',\n",
       " 'tf43',\n",
       " 'tf40',\n",
       " 'tf111',\n",
       " 'tf46',\n",
       " 'tf47',\n",
       " 'tf44',\n",
       " 'tf45']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       high    medium       low  listing_id\n",
      "0  0.088130  0.729487  0.182383     7142618\n",
      "1  0.015809  0.026031  0.958160     7210040\n",
      "2  0.027295  0.412797  0.559909     7103890\n",
      "3  0.175998  0.695555  0.128447     7143442\n",
      "4  0.034727  0.364566  0.600706     6860601\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "bst = lgb.train(params, train_data, best_n_estimators)\n",
    "pred = bst.predict(test_X)\n",
    "out_df = pd.DataFrame(pred)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"lgb_04_25_v2_cv0.5135.csv\", index=False)\n",
    "print(out_df.head())\n",
    "\n",
    "# cv:0.531324 lb:0.53275\n",
    "# cv:0.518967 lb:0.51759\n",
    "# cv:0.517992 room_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 1000, max_features= 50, criterion= 'gini', min_samples_split= 4,\n",
    "                                    max_depth= 20, min_samples_leaf= 2, n_jobs = -1, verbose = 1, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_X).isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X.fillna(-1).replace(np.inf, -1)\n",
    "test_X = test_X.fillna(-1).replace(np.inf, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "etc.fit(train_X, train_y)\n",
    "et_preds = etc.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       high    medium       low  listing_id\n",
      "0  0.095636  0.482819  0.421545     7142618\n",
      "1  0.007983  0.041947  0.950070     7210040\n",
      "2  0.018121  0.157359  0.824520     7103890\n",
      "3  0.238815  0.480919  0.280266     7143442\n",
      "4  0.051466  0.295523  0.653011     6860601\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame(et_preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"cv_et_n1000.csv\", index=False)\n",
    "print(out_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.3s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.58372008, -0.55499723, -0.55287353, -0.55738689, -0.5788555 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 100, max_features= 50, criterion= 'gini', min_samples_split= 4,\n",
    "                                    max_depth= 20, min_samples_leaf= 2, n_jobs = -1, verbose = 1, random_state = 5)\n",
    "score = cross_validation.cross_val_score(etc, train_X, train_y, scoring='log_loss', cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57336414428019888"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean() * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   36.4s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   37.9s finished\n",
      "C:\\Users\\hyunor\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.56904557, -0.57537624])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=200, min_samples_leaf = 2, min_samples_split = 2,\n",
    "                             max_features=50, max_depth=30, verbose = 1)\n",
    "score = cross_validation.cross_val_score(rfc, train_X, train_y, scoring='log_loss', cv=2)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       high    medium       low  listing_id\n",
      "0  0.143894  0.657117  0.198988     7142618\n",
      "1  0.011512  0.022761  0.965727     7210040\n",
      "2  0.008286  0.166035  0.825680     7103890\n",
      "3  0.191847  0.617181  0.190973     7143442\n",
      "4  0.070781  0.320497  0.608721     6860601\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame(preds * 0.8 + et_preds * 0.2)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"cv_lgb_et.csv\", index=False)\n",
    "print(out_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
